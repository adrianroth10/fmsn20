%% Dokumentinställningar börjar här

\documentclass[a4paper,english]{article}
%\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{cmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[perpage,symbol]{footmisc}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tocloft}
\usepackage{fancyhdr}
\setcounter{tocdepth}{2}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{babel}
\usepackage{setspace}
\usepackage{caption}
\usepackage{color}
%\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{afterpage}
\usepackage{verbatim}
\usepackage{ulem}
\usepackage{esint}
%\usepackage{icomma}     %% För att visa kommatecken i ekvationer korrekt
\usepackage{bold-extra} %% Extra teckensnitt för att kunna kombinera textsc och textbf
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{amsmath,mathdots}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage[framed,numbered]{matlab-prettifier}
\usepackage{filecontents}
\usepackage[left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{slantsc}
\usepackage{epstopdf}
\usepackage{dsfont}
\onehalfspacing

%%%%%%%%%%%%% - PDF-inställningar - %%%%%%%%%%%%%%
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{pdftitle={GMRF\_home\_assignment\_3},
 pdfauthor={Adrian Roth \& Anna Svensson},
 pdfsubject={Home assignment 3}}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% - Referenser - %%%%%%%%%%%%%%%%%
 \usepackage{csquotes}
 \renewcommand{\thefootnote}{\arabic{footnote}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatletter

%%%%%%%%%%%%%% - Fothuvud & Fotnot - %%%%%%%%%%%%%
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\scshape\nouppercase{Roth \& Svensson}}
%\fancyhead[RE]{\scshape\nouppercase{Roth \& Svensson}}
%\fancyhead[LE]{\scshape\nouppercase{\leftmark}}
\fancyhead[RO]{\scshape\nouppercase{\rightmark}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%% - Jakobs matteinställningar - %%%%%%%%%%
\renewcommand\cftsecleader{\cftdotfill{\cftdotsep}}
\renewcommand\thempfootnote{\fnsymbol{mpfootnote}}
\renewcommand\phi{\varphi}
\renewcommand\epsilon{\varepsilon}
\renewcommand\div{\mathrm{div}}
\def\tg{\tan}
\def\arctg{\arctan}
\newcommand{\jvec}[1]{\boldsymbol{\vec{#1}}}
\newcommand{\sexion}[1]{\section{#1}}
\newcommand{\subsexion}[1]{\subsection{#1}}
\newcommand{\subsubsexion}[1]{\subsubsection{#1}}
\newcommand{\high}[1]{\text{\raisebox{0.6ex}{$#1$}}}
\newcommand{\higher}[1]{\text{\raisebox{1.5ex}{$#1$}}}
\renewcommand\lstlistingname{\textsc{Matlab}--fil}
\renewcommand\lstlistlistingname{\textsc{Matlab}--filer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%% CENTRERA STORA BILDER %%%%%%%%%%%%%%
\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother
\renewcommand\refname{}
%% Dokumentinställningar slutar här

\begin{document}
\afterpage{\cfoot{\thepage}}

\title{\textit{{\textbf{\textsc{Spatial statistics with image analysis  \\ Home assignment 3 \\  MRF:s Classification}}}}}

\author{Adrian Roth \& Anna Svensson}

\maketitle
\thispagestyle{empty}



\pagebreak{}

\thispagestyle{empty}

\pagebreak{}

\section{Introduction}
In the research field of image analysis the problem of pixel classification is common.
For example it can be a foreground-background classification where it is interesting to classify which pixels are in the foreground and background respectively.
The simple approaches are the ones not concerning spatial dependence of pixel classes, i.e. a method which is not concerned if pixels spatially close in an image are classified as different classes.
In a real image it is reasonable and maybe even probable that this dependence exists.
Therefore it might be interesting to use a spatial dependent model for the classification.

In this assignment a sequence of fMRI images of a brain has been used.
The sequence contains images taken with 2 seconds intervals for 320 seconds in total (160 frames).
In the experiment the patient was either trying to judge if a pair of words rhymed for 20 seconds or rested for 20 seconds.
An example image of the brain when active and inactive respectively is found in Figure \ref{fig:intro:vis}.
The goal of the measurement is to classify which parts of the brain are active during the rhyme assessment.
There are two different versions of the data, one is the raw data explained above, the second one is a kind of regression of the data for each pixel to coefficients corresponding to different temporal functions.
The data set is then simplified from 160 temporal layers to 11 coefficient layers.
The first two temporal functions are constant and linear.
The coefficients for these functions will extract information of constant brain intensity and linear head movement during the 360 seconds.
The last eight functions are filters for the temporal intervals where the brain is active.
This means that the corresponding coefficients in $\beta$ should have information of which parts of the brain are active for this task.

For the classification three different methods are tested.
The K-means algorithm, Gaussian Mixture Model (GMM) and the Discrete Markov Random Field model (DMRF).
Only the DMRF method include spatial dependence, which should give it an advantage to the others.
Especially since the data is connected to neuron intensities.
For a simple model of the brain a neuron when active will send signals to neurons spatially close which have a higher probability of activating.
However, all models will be applied with different parameters for a discussion of which one captures the most important information of the fMRI data.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\textwidth]{../output/visualize1}
  \includegraphics[width=0.49\textwidth]{../output/visualize2}
  \caption{A visualisation of two data time frames. The left one is taken when the patient is actively trying to find rhymes and the right one is for an inactive part. No clear difference can be seen in these images.}
  \label{fig:intro:vis}
\end{figure}

\section{Theory}
The goal of the models introduced is to estimate the optimal parameters for each model.
The second part is to use these parameters to classify the pixels in the image.
This classification is referred to as the discrete latent field $\boldsymbol{x}$.
$\boldsymbol{x}$ is a grid with equal size as the image and each pixel variable in the field can have discrete values within the number of possible classes for the problem.
The latent field is strongly connected to the indicator field $\boldsymbol{z}$ where
\begin{equation}
  z_{ik} = \mathds{I}(x_i = k).
\end{equation}
$\boldsymbol{z}$ has as many layers as there are classes and each layer is either $1$ or $0$ where the sum of all layers for each pixel equals to $1$.
In other words, each pixel can only have one class.

\subsection{K-means}

K-Means is a computationally fast classification method where data is divided into clusters based on the assumption that there are equal amounts of all classes, $\pi_k = 1 / k$ as in Lecture 9, slide 20\cite{L09}. For \textit{K} classes, \textit{K} points are selected at random which are the initial cluster centres. Each data point is assigned to the closest $L^2$ norm cluster centre. The mean of each cluster is calculated and interpreted as the new cluster centre. This is repeated until convergence. The main advantage of K-means is that it is fast, simple and classifies directly, but the assumptions for which the method is based on might often not be realistic for the problem.

\subsection{Gaussian Mixture Model (GMM)}
The GMM can generally be found when dealing with classification problems.
It is convenient since it includes different classes automatically in the model.
The probability distribution is a sum of multivariate or univariate normal distributions where each is multiplied by a normalizing constant as
\begin{equation}
  \boldsymbol{y} | \boldsymbol{\theta} \in \sum_{k = 0}^K \pi_k \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
  \label{eq:gmm}
\end{equation}
$\boldsymbol{\theta}$ includes all expected values $\boldsymbol{\mu}_k$ and covariance matrices $\boldsymbol{\Sigma}_k$ and $\pi_k$ is the normalizing constant which represents the prior abundance of each class relative to the other $[1..K]$ classes.

The parameter maximum log likelihood estimation for assumed independent data is
\begin{equation}
  \hat{\boldsymbol{\theta}} = \text{argmax}_{\boldsymbol{\theta}} \sum_{i = 1}^n \log \left(  \sum_{k=1}^K \pi_k p_G(y_i | \boldsymbol{\theta}_k) \right)
\end{equation}
which is a hard problem to solve.
To make it easier we will use a method called Gibbs sampling to reduce the problem into simpler sub problems.
In general Gibbs sampling is a special case of a Markov Chain Monte Carlo (MCMC) method.
The basic idea of the MCMC method is to instead of doing the hard problem you sample from a probability density which have the same stationary distribution as the unknown hard density.
Though the samples are dependent a mean will still estimate expected value.
The Gibbs trick for the GMM is to divide the stochastic parameters into two blocks where each block is sampled conditional on the rest.
The two sub problems is to first sample from the pixel classification conditional on the parameters.
Secondly the parameters $\boldsymbol{\theta}$ for the GMM is sampled conditional on the pixel classes (Lecture 10 slide 17\cite{L10}).

\subsection{Discrete Markov Random Field (DMRF)}
A Discrete Markov Random field is defined by \textbf{x} where \textbf{x} fulfils the Markov condition as in equation \ref{eq:markov} where $\mathcal{N}_i$ are neighbours to a point $\textbf{s}_i$.
\begin{equation}
	p(x_i|\{x_j : j\neq i\}) = p(x_i|\{x_j : j\in \mathcal{N}_i\})
	\label{eq:markov}
\end{equation}
For gridded data neighbours can for example be the four closest or the eight closest neighbours to a pixel.
The field therefore includes a spatial dependence. For a Discrete Markov Random Field, \textbf{x} can only take values $x_i \in 1,2,...,K$ where \textit{K} is the number of classes. In the lecture slides the conditional distribution of $x_i$ is derived from the Gibbs distribution which includes the Markov condition. The conditional distribution is
\begin{equation}
  p(x_i|x_j,j\in \mathcal{N}_i) = \dfrac{\text{exp}(\alpha_{x_i}+\beta_{(x_i)}\sum_{j\in \mathcal{N}_i}\mathds{I}(x_j = x_i))}{\sum^K_{k=1}\text{exp}(\alpha_k + \beta_{(k)}\sum_{j\in \mathcal{N}_i}\mathds{I}(x_j = k))}
\end{equation}
where $\alpha$ is connected to how common a class \textit{k} is and $\beta_{(k)}$ is how similar neighbours in the same class $k$ are. The parenthesis around $x_i$ or $k$ in $\beta$ is explained by that the model can either have one $\beta$ for all classes or one for either one. After rewriting the indicator field as $z_{ik} = \mathds{I}(x_i = k)$ where $f_{ik} = \sum_{j\in \mathcal{N}_i} z_{jk}$ the expression is
\begin{equation}
  p(x_i|x_j,j\in \mathcal{N}_i) = \dfrac{\text{exp}(\alpha_{x_i}+\beta_{(x_i)} f_{ix_i})}{\sum^K_{k=1}\text{exp}(\alpha_k + \beta_{(k)} f_{ik})}.
\end{equation}
The approximate log likelihood of $\alpha$ and $\beta$ is
\begin{equation}
  \log PL_z(\boldsymbol{\alpha},\beta) = \sum_k \sum_i \alpha_{k} z_{ik} + \sum_k \beta_{(k)} \sum_i z_{ik}f_{ik} - \log(\sum_k \exp(\alpha_{k} + \beta_{(k)} f_{ik})).
\end{equation}
To the following posterior for $\boldsymbol{x}$ includes the data in the model,
\begin{equation}
  \begin{split}
    p(x_i|x_j,j\in \mathcal{N}_i, \boldsymbol{y}, \boldsymbol{\theta}) &\propto \exp (\tilde{\alpha}_{i, x_i}+\beta_{x_i} f_{ix_i}), \\
    \tilde{\alpha}_{i, x_i} &= \alpha_k + \log p(y_i | x_i = k, \boldsymbol{\theta}).
  \end{split}
\end{equation}
$\boldsymbol{\theta}$ in this case will include the parameters for the GMM of the data model.

To optimize all parameters, $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$ and GMM parameters, a Gibbs sampling approach is used.
The blocks are:
\begin{itemize}
  \item The indicator field $\boldsymbol{z}$. It is updated using a DMRF sampling method.
  \item The data GMM parameters are updated using the same approach as in the GMM Gibbs sampler.
  \item The $\alpha$:s and $\beta$:s are updated using a random walk Metropolis Hastings sampling.
    The samples are drawn from a distribution with a certain variance $\sigma_{MH}^2$.
    This sample is either accepted or disregarded through a likelihood calculation.
    The variance $\sigma_{MH}^2$ should be just right to get a ``good'' acceptance rate around $0.3$.
    It should also be the case of good mixing, visualized in Lecture 12, slide 21\cite{L12}.
\end{itemize}
The sampler will only after a period of burn in samples, sample the parameters from the correct distribution, this means that all samples in the burn in are extinguished and the samples afterwards are used to estimate the parameters' expected values.


\section{Method}
In this assignment the three different models for classification mentioned above has been used on the fMRI data.
The work has been focused on analysing the classification of the active parts of the brain which is hard to find just from the images in Figure \ref{fig:intro:vis}.
To exclude the uninteresting parts of the data only information from the last nine coefficients from the data regression $\beta$ (not the same $\beta$ as used in the DMRF) is used.
The assumption is made that all or at least a sufficient amount of the interesting data concerning the experimental active parts of the brain are stored in these coefficients.
All methods have been tried to classify the data though the main focus lies on the DMRF method since it is the only one including spatial dependence, an important component for brain activity intensity modelling.
The parameters which have been optimized are:
\begin{itemize}
  \item Number of classes for all methods.
  \item Number of principal components used.
  \item (Only for DMRF) The neighbourhood structure.
  \item (Only for DMRF) One $\beta$ parameter in total or one for each class.
\end{itemize}

\section{Results and discussion}

As mentioned in the Method section, the last nine covariate images were used for the classification as these should include the interesting information of the brain activity when finding rhymes. In Figure \ref{fig:mean} the mean of the relevant images is plotted, with indicated areas where the brain seems to have some kind of activity. This gives us a pointer on where to look for activity.

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/meanactivity}
    \caption{The mean image of the covariates which are relevant for detecting active parts of the brain. These are the last nine layers of the regression coefficients. Marked areas are where the authors guess that the activity is located.}
    \label{fig:mean}
\end{figure}

For the computationally fast method, K-means, the classification was made for all combinations of 2-7 classes and 1-5 principal components. The results were not very good but gave an indicator of where the active parts were in the brain. Too many classes made the image so noisy that is was nearly impossible to distinguish the interesting parts. An example of the best results is found in Figure \ref{fig:kmeans}.

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/kmeans_1_3}
    \caption{The results from one of the classifications made with the K-means method. This example was created with three classes and only the first principal component. The image is very noisy but kind of indicates where the active parts are. Note that this is one of the best images, in the authors opinion.}
    \label{fig:kmeans}
\end{figure}

The Gaussian Mixture Model was used with the same classes and principal components as K-means. The results were slightly less noisy but for most of the combinations, you need to know where to look to actually see the active parts. One of the best images can be seen in Figure \ref{fig:gmm}.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.7\linewidth]{../output/gmm_1_3}
  \caption{The results from one of the classifications made with the Gaussian Mixture Model. This example was created with three classes and the first principal component. The image is rather noisy but kind of indicates where the active parts is. Again, this is the best classification in the authors opinion.}
  \label{fig:gmm}
\end{figure}

In the DMRF Gibbs sampling for optimizing the parameters, 1000 iterations with a burn in of 300 was used and as mentioned there is a Metropolis Hastings sampling stage with a variance parameter  $\sigma_{MH}^2$.
The optimum of this parameter, concerning acceptance rate (around $0.3$) and good mixing, is dependent on all of the parameters named in the Method section.
This means that is should be optimized for each and every one of them.
Trying $n$ multiple ones of each results in combinations that gets larger with $n^4$ and to optimize these is an unreasonable task, even for $n=4$, with the time for this project.
In the end we chose 3 different number of classes, 4 number of principal components, 2 neighbourhood structures and 2 versions of the beta parameter (either one for all classes or one for each class), this gives 48 different $\sigma_{MH}^2$ to be optimized.
The upper limitation for number of classes was based on visual assessments, seen in the plots of $\alpha$:s and $\beta$:s from each iteration, similar to the ones in Figure \ref{fig:alphabeta} it seemed as though the $\alpha$:s got to a more stationary distribution for the runs with fewer number of classes.
We decided to use four principal components from observing the eigenvalues gained from the principal component analysis, see figure \ref{fig:pca}. The first eigenvalues are the most significant and as a second reason we chose four to limit the combinations. The last mentioned is also why only two neighbourhood structures and versions of beta parameters were used.

Looking at plots similar to the ones in Figure \ref{fig:alphabeta} the $\alpha$ and $\beta$ parameters are discussed.
When many classes were used, the $\alpha$:s almost had an oscillating appearance. Some plots did not follow this assessment, but it was a vague assertion that was made.
It can be explained by having too many classes compared to how many exists in the data makes it harder to classify more than that. In our case there might be two classes, active and non-active areas, then it is problematic to classify more than two types of data.
If there are no other obvious classes the classifier will try to classify similar data and this will make $\alpha$ vary a lot since the classification is easily transferred in the iterations. There is no obvious differences for how the $\alpha$:s behave for different neighbourhood structures, but if a single $\beta$ or different $\beta$:s for each class had been used, it seemed to affect the $\alpha$:s.
For different $\beta$:s for each class the $\alpha$:s converged slower, or not at all during the iterations used.
If we would have used more iterations we might have got better results for the convergence.
Different $\beta$:s gives more dependent parameters to take in to account which could be the reason for the slow convergence.
The $\beta$:s converged quickly in general, especially for more principal components.
Though with more classes it converged slower, in particular for few principal components.
It makes sense since with a similar explanation as for the $\alpha$:s, the classes are transferred which might slow the convergence.
The result that $\beta$ converges faster with more principal components, could be explained with that more data is used, but it is peculiar that this behaviour does not seem to show in the $\alpha$ plots where it also should converge faster if more data is provided.
When comparing different neighbourhoods and different $\beta$:s for each class the results were more or less the same as for the $\alpha$:s, there are no clear differences for the neighbourhood structures, but they converge faster if only one $\beta$ for all classes is used.
But is it reasonable that it is better to only have a single $\beta$ for all classes?
It could be argued that the background of the image should have a larger dependence compared to the active brain parts since parts of the background are outside of the head where everything is background.
The reason for our results could be connected to how we have optimized the acceptance rate for the sampling which is discussed later or maybe because of too few iterations and burn in for the sampling to converge to the stationary distributions.

Looking at the result in Figure \ref{fig:plog} the pseudo log likelihood is slightly larger for few classes used, independent of how many used principal components. This, in addition to the discussion above, suggests that it might be a good idea to use few classes for the evaluation. Visual results of the classification also showed that, from our point of view, the active parts had been best classified for few classes. An example of the results using two classes is found in Figure \ref{fig:mrf}.

Using the same $\sigma_{MH}^2$ for both $\beta$ structures seemed to give okay results. Sometimes the results are quite bad, the reader should be aware of that the results when using one $\beta$ for each class is not optimised and might be worse because of that. Though the acceptance for the other three variables mentioned in the method part was problematic. 12 $\sigma_{MH}^2$ were tuned for each neighbourhood structure, resulting in 24 different parameters to be manually optimized.
We say that a good $\sigma_{MH}^2$ is one that results in acceptance rate of $0.3 \pm 0.05$ since the result of good mixing is more subjective from what we know.
The $\sigma_{MH}^2$:s and the corresponding acceptance rate is found in Tables \ref{tab:sigma:n1}, \ref{tab:sigma:n2}, \ref{tab:acceptance:n1} and \ref{tab:acceptance:n2}.
From the information of the $\sigma_{MH}^2$:s there seems to be some connection with lower $\sigma_{MH}^2$ and more principal components or larger neighbourhood.
This suggests that larger neighbourhoods and more principal components are connected to smaller steps in the Metropolis Hastings random walk of the $\alpha$ and $\beta$ parameters.
However we can not explain how this is connected to the posterior distributions of $\alpha$ and $\beta$ with our current knowledge level.

As a conclusion we find that the best versions of each of the methods very similarly, and more or less correctly, classifies the parts of the brain which are indicated in Figure \ref{fig:mean} differently compared to the background (our subjective opinion).
The difference is how the DMRF is less noisy in its classification which is why we also believe that this is the best one.
Interesting to note from this classification is that the back parts of the brain corresponds to the visual cortex.
For a patient which visually tries to read possible rhymes it is reasonable that it should be active.
That leaves brain activity for understanding what is read and if it is rhyming.
The results suggests that the reading and/or rhyme-centre of the brain is found to be either in the front or front-right part of the brain.


\begin{table}[H]
  \centering
  \begin{tabular}{c c c}
    \input{../output/mhsigman1.tex}
  \end{tabular}
  \caption{The table includes the optimized $\sigma_{MH}^2$, corresponding to an acceptance rate of $0.3 \pm 0.05$, for each combination of principal components used ${[1], [1,2], [1,2,3], [1,2,3,4]}$ as rows and number of classes $[2, 3, 4]$ as columns. This is optimized for the 4 neighbourhood and a single $\beta$ parameter. It is hard to see a connection between the covariates of number of classes and the used principal components though it seems that $\sigma_{MH}^2$ gets lower when using more principal components.}
  \label{tab:sigma:n1}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{c c c}
    \input{../output/mhsigman2.tex}
  \end{tabular}
  \caption{The same content as Table \ref{tab:sigma:n1} with the difference of using 8 neighbourhood instead of 4.}
  \label{tab:sigma:n2}
\end{table}


\begin{table}[H]
  \centering
  \begin{tabular}{c c c}
    \input{../output/acceptance_rate_1_2_1_1.tex} & \input{../output/acceptance_rate_1_3_1_1.tex} & \input{../output/acceptance_rate_1_4_1_1.tex} \\
    \input{../output/acceptance_rate_12_2_1_1.tex} & \input{../output/acceptance_rate_12_3_1_1.tex} & \input{../output/acceptance_rate_12_4_1_1.tex} \\
    \input{../output/acceptance_rate_123_2_1_1.tex} & \input{../output/acceptance_rate_123_3_1_1.tex} & \input{../output/acceptance_rate_123_4_1_1.tex} \\
    \input{../output/acceptance_rate_1234_2_2_1.tex} & \input{../output/acceptance_rate_1234_3_1_1.tex} & \input{../output/acceptance_rate_1234_4_2_1.tex} \\
  \end{tabular}
  \caption{The acceptance rates for the corresponding $\sigma_{MH}^2$ as in Table \ref{tab:sigma:n1}. They are optimised to lie within the range $[0.25 - 0.35]$.}
  \label{tab:acceptance:n1}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{c c c}
    \input{../output/acceptance_rate_1_2_2_1.tex} & \input{../output/acceptance_rate_1_3_2_1.tex} & \input{../output/acceptance_rate_1_4_2_1.tex} \\
    \input{../output/acceptance_rate_12_2_2_1.tex} & \input{../output/acceptance_rate_12_3_2_1.tex} & \input{../output/acceptance_rate_12_4_2_1.tex} \\
    \input{../output/acceptance_rate_123_2_2_1.tex} & \input{../output/acceptance_rate_123_3_2_1.tex} & \input{../output/acceptance_rate_123_4_2_1.tex} \\
    \input{../output/acceptance_rate_1234_2_2_1.tex} & \input{../output/acceptance_rate_1234_3_2_1.tex} & \input{../output/acceptance_rate_1234_4_2_1.tex} \\
  \end{tabular}
  \caption{The acceptance rates for the corresponding $\sigma_{MH}^2$ as in Table \ref{tab:sigma:n2}. They are optimised to lie within the range $[0.25 - 0.35]$.}
  \label{tab:acceptance:n2}
\end{table}

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/pdata}
    \caption{The eigenvalues of the principal component analysis shows that the first few components are the most significant ones.}
    \label{fig:pca}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\textwidth]{../output/mrf_alpha_12_2_2_1}
  \includegraphics[width=0.49\textwidth]{../output/mrf_beta_12_2_2_1}
  \caption{An example of the plots of $\alpha$:s and $\beta$:s, corresponding to the run with two principal components, two classes, the 8-neighbourhood structure and the same $\beta$:s for both classes. The $\alpha$-value seems to converge around a value slightly larger than 0, and the $\beta$ to a value around 0.55. They converge quite quickly and with good mixing which is also good results for the model. Burn in was put up to the 300:th iteration, and they seem to have converged, more or less, before this.}
  \label{fig:alphabeta}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/mrf_plog_12_2_2_1}
    \caption{The pseudo likelihood function for the DMRF estimation corresponding to the results in Figures \ref{fig:alphabeta} and \ref{fig:mrf}. It converges quite fast to a larger likelihood compared to estimations for different models.}
    \label{fig:plog}
\end{figure}


\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/mrf_12_2_2_1}
    \caption{The result of the classification using two principal components, two classes, the 8-neighbourhood structure and the same $\beta$:s for both classes. As the model is dependent of neighbours, it is a lot less noisy compared to the results of the other models tried in this assignment. It can also be seen that the classifier finds activity at parts of the brain which were indicated in Figure \ref{fig:mean} together with some curious checkerboard patterns found in the original data in Figure \ref{fig:intro:vis} which we can not explain.}
    \label{fig:mrf}
\end{figure}

\newpage

\begin{thebibliography}{37}
  \bibitem{L09}\textbf{Lecture L09} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L09-1x3.pdf}
  \bibitem{L10}\textbf{Lecture L10} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L10-1x3.pdf}
  \bibitem{L12}\textbf{Lecture L12} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L12-1x3.pdf}
\end{thebibliography}
\pagebreak{}

\pagebreak{}
\thispagestyle{empty}

\end{document}
