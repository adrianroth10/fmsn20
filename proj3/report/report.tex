%% Dokumentinställningar börjar här

\documentclass[a4paper,english]{article}
%\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{cmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[perpage,symbol]{footmisc}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tocloft}
\usepackage{fancyhdr}
\setcounter{tocdepth}{2}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{babel}
\usepackage{setspace}
\usepackage{caption}
\usepackage{color}
%\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{afterpage}
\usepackage{verbatim}
\usepackage{ulem}
\usepackage{esint}
%\usepackage{icomma}     %% För att visa kommatecken i ekvationer korrekt
\usepackage{bold-extra} %% Extra teckensnitt för att kunna kombinera textsc och textbf
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{amsmath,mathdots}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage[framed,numbered]{matlab-prettifier}
\usepackage{filecontents}
\usepackage[left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{slantsc}
\usepackage{epstopdf}
\usepackage{dsfont}
\onehalfspacing

%%%%%%%%%%%%% - PDF-inställningar - %%%%%%%%%%%%%%
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{pdftitle={GMRF\_home\_assignment\_3},
 pdfauthor={Adrian Roth \& Anna Svensson},
 pdfsubject={Home assignment 3}}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% - Referenser - %%%%%%%%%%%%%%%%%
 \usepackage{csquotes}
 \renewcommand{\thefootnote}{\arabic{footnote}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatletter

%%%%%%%%%%%%%% - Fothuvud & Fotnot - %%%%%%%%%%%%%
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\scshape\nouppercase{Roth \& Svensson}}
%\fancyhead[RE]{\scshape\nouppercase{Roth \& Svensson}}
%\fancyhead[LE]{\scshape\nouppercase{\leftmark}}
\fancyhead[RO]{\scshape\nouppercase{\rightmark}}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%% - Jakobs matteinställningar - %%%%%%%%%%
\renewcommand\cftsecleader{\cftdotfill{\cftdotsep}}
\renewcommand\thempfootnote{\fnsymbol{mpfootnote}}
\renewcommand\phi{\varphi}
\renewcommand\epsilon{\varepsilon}
\renewcommand\div{\mathrm{div}}
\def\tg{\tan}
\def\arctg{\arctan}
\newcommand{\jvec}[1]{\boldsymbol{\vec{#1}}}
\newcommand{\sexion}[1]{\section{#1}}
\newcommand{\subsexion}[1]{\subsection{#1}}
\newcommand{\subsubsexion}[1]{\subsubsection{#1}}
\newcommand{\high}[1]{\text{\raisebox{0.6ex}{$#1$}}}
\newcommand{\higher}[1]{\text{\raisebox{1.5ex}{$#1$}}}
\renewcommand\lstlistingname{\textsc{Matlab}--fil}
\renewcommand\lstlistlistingname{\textsc{Matlab}--filer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%% CENTRERA STORA BILDER %%%%%%%%%%%%%%
\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatother
\renewcommand\refname{}
%% Dokumentinställningar slutar här

\begin{document}
\afterpage{\cfoot{\thepage}}

\title{\textit{{\textbf{\textsc{Spatial statistics with image analysis  \\ Home assignment 3 \\  MRF:s Classification}}}}}

\author{Adrian Roth \& Anna Svensson}

\maketitle
\thispagestyle{empty}



\pagebreak{}

\thispagestyle{empty}

\pagebreak{}

\section{Introduction}
In the research field of image analysis the problem of pixel classification is a common.
For example it can be a foreground-background classification where it is interesting to classify which pixels are in the foreground and background respectively.
The simple approaches are ones not concerning spatial dependence of pixel classes, i.e. a method which is not concerned if pixels spatially close in an image are classified as different classes.
In a real image it is reasonable and maybe even probable that this dependence exists.
Therefore it might be interesting to use a spatial dependent model for the classification.

In this assignment a sequence of fMRI images of an brain has been used.
The sequence contains images taken with 2 seconds intervals for 320 seconds total (160 frames).
In the experiment the patient was either trying to judge if a pair of words rhymed for 20 seconds or rested for 20 seconds.
An example image of the brain when active and inactive respectively is found in Figure \ref{fig:intro:vis}.
The goal of the measurement is to classify which parts of the brain are active during the rhyme assessment.
There are two different versions of the data, one is the raw data explained above, the second one is a kind of regression of the data for each pixel to coefficients $\beta$ corresponding to different temporal functions.
The data set is then simplified from 160 temporal layers to 11 coefficient layers.
The first two temporal functions are constant and linear.
The coefficients for these functions will extract information of constant brain intensity and linear head movement during the 360 seconds.
The last eight functions are filters for the temporal intervals where the brain is active.
This means that the corresponding coefficients in $\beta$ should have information of which parts of the brain are active for this task.

For the classification three different methods are tested.
The K-means algorithm, Gaussian Mixture Model (GMM) and the Discrete Markov Random Field model (DMRF).
Only the DMRF method include spatial dependence, which should give it an advantage to the others.
Especially since the data is connected to neuron intensities.
For a simple model of the brain a neuron when active will send signals to neurons spatially close which have a higher probability of activating.
However, all models will be applied with different parameters for a discussion of which one captures the most important information of the fMRI data.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\textwidth]{../output/visualize1}
  \includegraphics[width=0.49\textwidth]{../output/visualize2}
  \caption{A visualisation of two data time frames. The left one is taken when the patient is actively trying to find rhymes and the right one is for an inactive part. No clear difference can be seen in these images.}
  \label{fig:intro:vis}
\end{figure}

\section{Theory}
The goal of the models introduced is to estimate both the optimal parameters for each model.
The second part is to use these parameters to classify the pixels in the image.
This classification is referred to as the discrete latent field $\boldsymbol{x}$.
$\boldsymbol{x}$ is a grid with equal size as the image and each pixel variable in the field can have discrete values within the number of possible classes for the problem.
The latent is field strongly connected to the indicator field $\boldsymbol{z}$ where
\begin{equation}
  z_{ik} = \mathds{I}(x_i = k).
\end{equation}
$\boldsymbol{z}$ has as many layers as there are classes and each layer is either $1$ or $0$ where the sum of all layers for each pixel equals to $1$.
In other words, each pixel can only have one class.

\subsection{K-means}

K-Means is a computationally fast classification method where data is divided into clusters based on the assumption that there are equal amounts of all classes, $\pi_k = 1 / k$ as in Lecture 9, slide 20\cite{L09}. For \textit{K} classes, \textit{K} points are selected at random which are the initial cluster centres. Each data point is assigned to the closest $L^2$ norm cluster centre. The mean of each cluster is calculated and interpreted as the new cluster centre. This is repeated until convergence. The main advantage of K-means is that it is fast, simple and classifies directly, but the assumptions for which the method is based on might often not be realistic for the problem.

\subsection{Gaussian Mixture Model (GMM)}
The GMM can generally be found when dealing with classification problems.
It is convenient since it includes different classes automatically in the model.
The probability distribution is a sum of multivariate or univariate normal distributions where each is multiplied by a normalizing constant as
\begin{equation}
  \boldsymbol{y} | \boldsymbol{\theta} \in \sum_{k = 0}^K \pi_k \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
  \label{eq:gmm}
\end{equation}
$\boldsymbol{\theta}$ includes all expected values $\boldsymbol{\mu}_k$ and covariance matrices $\boldsymbol{\Sigma}_k$ and $\pi_k$ is the normalizing constant which represents the prior abundance of each class relative to the other $[1..K]$ classes.

The parameter maximum log likelihood estimation for assumed independent data is
\begin{equation}
  \hat{\boldsymbol{\theta}} = \text{argmax}_{\boldsymbol{\theta}} \sum_{i = 1}^n \log \left(  \sum_{k=1}^K \pi_k p_G(y_i | \boldsymbol{\theta}_k) \right)
\end{equation}
which is a hard problem to solve.
To make it easier we will use a method called Gibbs sampling to reduce the problem into simpler sub problems.
In general Gibbs sampling is a special case of a Markov Chain Monte Carlo (MCMC) method.
The basic idea of the MCMC method is to instead of doing the hard problem you sample from a probability density which have the same stationary distribution as the unknown hard density.
Though the samples are dependent a mean will still estimate expected value.
The Gibbs trick for the GMM is to divide the stochastic parameters into two blocks where each block is sampled conditional on the rest.
The two sub problems is to first sample from the pixel classification conditional on the parameters.
Secondly the parameters $\boldsymbol{\theta}$ for the GMM is sampled conditional on the pixel classes (Lecture 10 slide 17\cite{L10}).

\subsection{Discrete Markov Random Field (DMRF)}
A Discrete Markov Random field is defined by \textbf{x} where \textbf{x} fulfils the Markov condition as in equation \ref{eq:markov} where $\mathcal{N}_i$ are neighbours to a point $\textbf{s}_i$.
\begin{equation}
	p(x_i|\{x_j : j\neq i\}) = p(x_i|\{x_j : j\in \mathcal{N}_i\})
	\label{eq:markov}
\end{equation}
For gridded data neighbours can for example be the four closest or the eight closest neighbours to a pixel.
The field is therefore includes a spatial dependence. For a Discrete Markov Random Field, \textbf{x} can only take values $x_i \in 1,2,...,K$ where \textit{K} is the number of classes. In the lecture slides the conditional distribution of $x_i$ is derived from the Gibbs distribution which includes the Markov condition. The conditional distribution is
\begin{equation}
  p(x_i|x_j,j\in \mathcal{N}_i) = \dfrac{\text{exp}(\alpha_{x_i}+\beta_{(x_i)}\sum_{j\in \mathcal{N}_i}\mathds{I}(x_j = x_i))}{\sum^K_{k=1}\text{exp}(\alpha_k + \beta_{(k)}\sum_{j\in \mathcal{N}_i}\mathds{I}(x_j = k))}
\end{equation}
where $\alpha$ and is connected to how common a class \textit{k} is and $\beta_{(k)}$ is how similar neighbours in the same class $k$ are. The parenthesis around $x_i$ or $k$ in $\beta$ is explained by that the model can either have one $\beta$ for all classes or one for either one. After rewriting the indicator field as $z_{ik} = \mathds{I}(x_i = k)$ where $f_{ik} = \sum_{j\in \mathcal{N}_i} z_{jk}$ the expression is
\begin{equation}
  p(x_i|x_j,j\in \mathcal{N}_i) = \dfrac{\text{exp}(\alpha_{x_i}+\beta_{(x_i)} f_{ix_i})}{\sum^K_{k=1}\text{exp}(\alpha_k + \beta_{(k)} f_{ik})}.
\end{equation}
The approximate log likelihood of $\alpha$ and $\beta$ is
\begin{equation}
  \log PL_z(\boldsymbol{\alpha},\beta) = \sum_k \sum_i \alpha_{k} z_{ik} + \sum_k \beta_{(k)} \sum_i z_{ik}f_{ik} - \log(\sum_k \exp(\alpha_{k} + \beta_{(k)} f_{ik})).
\end{equation}
To the following posterior for $\boldsymbol{x}$ includes the data in the model,
\begin{equation}
  \begin{split}
    p(x_i|x_j,j\in \mathcal{N}_i, \boldsymbol{y}, \boldsymbol{\theta}) &\propto \exp (\tilde{\alpha}_{i, x_i}+\beta_{x_i} f_{ix_i}), \\
    \tilde{\alpha}_{i, x_i} &= \alpha_k + \log p(y_i | x_i = k, \boldsymbol{\theta}).
  \end{split}
\end{equation}
$\boldsymbol{\theta}$ in this case will include the parameters for the GMM of the data model.

To optimize all parameters, $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$ and GMM parameters, a Gibbs sampling approach is used.
The blocks are:
\begin{itemize}
  \item The indicator field $\boldsymbol{z}$. It is updated using a DMRF sampling method.
  \item The data GMM parameters are updated using the same approach as in the GMM Gibbs sampler.
  \item The $\alpha$:s and $\beta$:s are updated using a random walk Metropolis Hastings sampling.
    The samples are drawn from a distribution with a certain variance $\sigma_{MH}^2$.
    This sample is is either accepted or disregarded through a likelihood calculation.
    The variance $\sigma_{MH}^2$ should be just right to get a ``good'' acceptance rate around $0.3$.
    It should also be the case of good mixing, visualized in Lecture 12, slide 21\cite{L12}.
\end{itemize}
The sampler will only after a period of burn in samples, sample the parameters from the correct distribution, this means that all samples in the burn in are extinguished and the samples afterwards are used to estimate the parameters expected values.


\section{Method}
In this assignment the three different models for classification mentioned above has been used on the fMRI data.
The work has been focused on analysing the classification of the active parts of the brain which is hard to find just from the images in Figure \ref{fig:intro:vis}.
To exclude the uninteresting parts of the data only information from the last nine coefficients from the data regression $\beta$ is used.
The assumption is made that all or at least a sufficient amount of the interesting data concerning the experimental active parts of the brain are stored in these coefficients.
All methods have been tried to classify the data though the main focus lies on the DMRF method since it is the only one including spatial dependence, an important component for brain activity intensity modelling.
The parameters which have been optimized are:
\begin{itemize}
  \item Number of classes for all methods.
  \item Number of principal components used.
  \item (Only for DMRF) The neighbourhood structure.
  \item (Only for DMRF) One $\beta$ parameter in total or one for each class.
\end{itemize}

\section{Results and discussion}

As mentioned in the Method section, the last nine covariate images were used for the classification as these can describe the brain activity. In Figure \ref{fig:mean} the mean of the relevant images is plotted, with indicated areas where the brain seems to have some kind of activity. This gives us a pointer on where to look for activity.

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/meanactivity}
    \caption{}
    \label{fig:mean}
\end{figure}

For the quick method K-means the classification was made for 2-7 classes and for 1-5 covariate images. The results were not very good but gave an indicator of where the active parts were in the brain. Too many classes made the image so noisy that is was nearly impossible to distinguish the interesting parts. An example of the best results is found in Figure \ref{fig:kmeans}.

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/kmeans_1_3}
    \caption{}
    \label{fig:kmeans}
\end{figure}

The Gaussian Mixture Model was used with the same classes and covariates as K-means. The results were slightly less noisy but for most of the combinations, you need to know where to look to actually see the active parts. One of the best images can be seen in Figure \ref{fig:gmm}.


\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/gmm_1_6}
    \caption{}
    \label{fig:gmm}
\end{figure}


In the DMRF Gibbs sampling for optimizing the parameters there is as mentioned a Metropolis Hastings sampling stage with a variance parameter  $\sigma_{MH}^2$.
The optimum of this parameter, concerning acceptance rate (around $0.3$) and good mixing, is dependent on all of the parameters named in the Method section.
This means that is should be optimized for each and every one of them.
Trying $n$ multiple ones of each results in combinations that gets larger with $n^4$ and to optimize these is an unreasonable task for large $n$.
In the end we chose 3 different number of classes, 4 number of principal components, 2 neighbourhood structures and 2 versions of the beta parameter, this gives 48 different $\sigma_{MH}^2$ to be optimized. The limitation of classes was based on visual assessments, seen in the plots of $\alpha$:s and $\beta$:s from each iteration, similar to the ones in Figure \ref{fig:alphabeta} it seemed as though the $\alpha$:s were more stationary for the runs with few classes. They did not really converge to a value but they did converge around a value. Where many classes were used, the $\alpha$:s almost had an oscillating appearance. Some plots did not follow this assessment, but it was a vague assertion that was made.
It can be explained by having a lot of classes for data that mainly shows active and non-active areas makes it hard to classify more than two types of data. If there is no other obvious classes the classifier will try to classify similar data and this will make $\alpha$ vary a lot since the classification is easily overwritten in the iterations. The $\beta$:s converged quickly in general, especially if many covariate images were used. Where many classes were used it converged slowly, in particular for few covariate images. It makes sense since with a similar explanation as for the $alpha$:s, the classes are overwritten a lot of times, making the $\beta$:s vary. With many covariates, more data is given which could explain why it converges quickly, but it is peculiar that this behaviour does not seem to show in the $\alpha$ plots where it also should converge faster if more data is provided. The pseudo log likelihood is slightly bigger for few classes used, independent of how many covariates. This in addition to the discussion above suggests that it might be a good idea to use few classes for the evaluation. Visual results of the classification also showed that, from our point of view, the active parts had been best classified for few classes. An example of the results using two classes is found in Figure \ref{fig:mrf}.

We decided to use four principal components from observing the eigenvalues gained from the principal component analysis, see figure \ref{fig:pca}. The first eigenvalues are the most significant and as a second reason we chose four to limit the combinations. The last mentioned is also why only two neighbourhood structures and versions of beta parameters were used.
Using the same $\sigma_{MH}^2$ for both $\beta$ structures seemed to give okay results, while it was problematic for the different neighbourhood structures. 12 $\sigma_{MH}^2$ were tuned for each neighbourhood structure, resulting in 24 different parameters to be manually optimized.
We say that a good $\sigma_{MH}^2$ is one that results in acceptance rate of $0.3 \pm 0.1$.
The $\sigma_{MH}^2$:s and the corresponding acceptance rate is found in Tables \dots and \dots there seems to be some connection with lower $\sigma_{MH}^2$ and more principal components.

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/pdata}
    \caption{The eigenvalues of the principal component analysis shows that the first few components are most significant.}
    \label{fig:pca}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.49\textwidth]{../output/mrf_alpha_12_2_2_1}
  \includegraphics[width=0.49\textwidth]{../output/mrf_beta_12_2_2_1}
  \caption{}
  \label{fig:alphabeta}
\end{figure}

\begin{figure}[H]
	\centering
    \includegraphics[width = 0.7\linewidth]{../output/mrf_12_2_2_1}
    \caption{}
    \label{fig:mrf}
\end{figure}


\newpage

\begin{thebibliography}{37}
  \bibitem{L09}\textbf{Lecture L09} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L09-1x3.pdf}
  \bibitem{L10}\textbf{Lecture L10} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L10-1x3.pdf}
  \bibitem{L12}\textbf{Lecture L12} \\
\url{http://www.maths.lth.se/matstat/kurser/fmsn20masm25//material_ht18/L12-1x3.pdf}
\end{thebibliography}
\pagebreak{}

\sexion{Appendix}
\begin{figure}[H]
  \centering
  %\includegraphics[width=\linewidth]{../output/kmeans_1_2_1}
  \caption{testing}
  \label{fig:test}
\end{figure}


%\subsection*{\textsc{Matlab} code}
% \input{kod}


\pagebreak{}
\thispagestyle{empty}

\end{document}
